@book{knuth2001art,
    title={The Art of Computer Programming: Fundamental Algorithms},
    author={Knuth, D.E.},
    number={v. 1},
    isbn={9780201896831},
    series={The Art of Computer Programming: Fundamental Algorithms},
    year={2001},
    publisher={Addison-Wesley}
}
@INPROCEEDINGS{4026885,
    author={W. Vogels},
    booktitle={2006 IEEE International Conference on Services Computing (SCC'06)},
    title={Web Services at Amazon.com},
    year={2006},
    pages={xxii-xxii},
    keywords={Distributed computing;Technological innovation;Web services},
    doi={10.1109/SCC.2006.116},
    month={Sept}
}

@inproceedings{aws,
author = {Zhang, Ben and Jin, Xin and Ratnasamy, Sylvia and Wawrzynek, John and Lee, Edward A.},
title = {AWStream: adaptive wide-area streaming analytics},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230554},
doi = {10.1145/3230543.3230554},
abstract = {The emerging class of wide-area streaming analytics faces the challenge of scarce and variable WAN bandwidth. Non-adaptive applications built with TCP or UDP suffer from increased latency or degraded accuracy. State-of-the-art approaches that adapt to network changes require developer writing sub-optimal manual policies or are limited to application-specific optimizations.We present AWStream, a stream processing system that simultaneously achieves low latency and high accuracy in the wide area, requiring minimal developer efforts. To realize this, AWStream uses three ideas: (i) it integrates application adaptation as a first-class programming abstraction in the stream processing model; (ii) with a combination of offline and online profiling, it automatically learns an accurate profile that models accuracy and bandwidth trade-off; and (iii) at runtime, it carefully adjusts the application data rate to match the available bandwidth while maximizing the achievable accuracy. We evaluate AWStream with three real-world applications: augmented reality, pedestrian detection, and monitoring log analysis. Our experiments show that AWStream achieves sub-second latency with only nominal accuracy drop (2-6\%).},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {236–252},
numpages = {17},
keywords = {wide area network, profiling, learning, adaptation},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@inproceedings{dds,
author = {Du, Kuntai and Pervaiz, Ahsan and Yuan, Xin and Chowdhery, Aakanksha and Zhang, Qizheng and Hoffmann, Henry and Jiang, Junchen},
title = {Server-Driven Video Streaming for Deep Learning Inference},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405887},
doi = {10.1145/3387514.3405887},
abstract = {Video streaming is crucial for AI applications that gather videos from sources to servers for inference by deep neural nets (DNNs). Unlike traditional video streaming that optimizes visual quality, this new type of video streaming permits aggressive compression/pruning of pixels not relevant to achieving high DNN inference accuracy. However, much of this potential is left unrealized, because current video streaming protocols are driven by the video source (camera) where the compute is rather limited. We advocate that the video streaming protocol should be driven by real-time feedback from the server-side DNN. Our insight is two-fold: (1) server-side DNN has more context about the pixels that maximize its inference accuracy; and (2) the DNN's output contains rich information useful to guide video streaming. We present DDS (DNN-Driven Streaming), a concrete design of this approach. DDS continuously sends a low-quality video stream to the server; the server runs the DNN to determine where to re-send with higher quality to increase the inference accuracy. We find that compared to several recent baselines on multiple video genres and vision tasks, DDS maintains higher accuracy while reducing bandwidth usage by upto 59\% or improves accuracy by upto 9\% with no additional bandwidth usage.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {557–570},
numpages = {14},
keywords = {deep neural networks, feedback-driven, video analytics, video streaming},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{concierge,
author = {Huang, Yuyang and Zharfan, Faishal and Hendrawan and Gunawi, Haryadi S and Jiang, Junchen},
title = {Concierge: Towards Accuracy-Driven Bandwidth Allocation for Video Analytics Applications in Edge Network},
year = {2024},
publisher = {IEEE},
abstract = {When performing inference on sensor data, edge video analytics applications may not always need high-fidelity data, since important information may not appear all the time. Consequently, each edge AI application’s bandwidth demand is highly dynamic. Thus, a shared edge system should dynamically allocate more bandwidth to the applications that need more bandwidth to reach high accuracy at each moment. However, previous bandwidth allocators are ill-suited because they are agnostic to the time-varying impact of bandwidth on each application’s accuracy.},
booktitle = {2024 IEEE International Conference on Edge Computing {\&} Communications (EDGE 24)},
keywords = {deep neural networks, feedback-driven, video analytics, video streaming},
location = {Shenzhen, China}
}

@inproceedings {videostorm,
author = {Haoyu Zhang and Ganesh Ananthanarayanan and Peter Bodik and Matthai Philipose and Paramvir Bahl and Michael J. Freedman},
title = {Live Video Analytics at Scale with Approximation and {Delay-Tolerance}},
booktitle = {14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)},
year = {2017},
isbn = {978-1-931971-37-9},
address = {Boston, MA},
pages = {377--392},
url = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/zhang},
publisher = {USENIX Association},
month = mar
}

@misc{rtx6000, url={https://www.nvidia.com/en-us/design-visualization/rtx-6000/}, journal={NVIDIA}} 

@misc{tc,
publisher = {The Linux Documentation Project},
author = {{The Linux Documentation Project}},
title = {tc - tool for controlling traffic in Linux},
year = {2024}}

@misc{internet, 
title={Global average internet speed, 1990-2050}, 
url={https://futuretimeline.net/data-trends/2050-future-internet-speed-predictions.htm}, 
journal={Future Timeline}, 
author={Fox, Will}
} 

@misc{4g, author={Khalifa, Ehab}, url={https://www.4g.co.uk/how-fast-is-4g/}, journal={4G}, year={2024}} 

@misc{5g, title={What is 5G - an introduction to the technology}, year={2024}, url={https://www.reply.com/en/telco-and-media/5g-mastering-the-magic-triangle}, journal={Reply}, author={Reply, TamTamy}} 

@misc{edgeComp, author={Khalifa, Ehab}, url={https://www.ibm.com/topics/edge-computing}, journal={IBM}, year={2023}, month={Jun}} 

@ARTICLE{edgeComp2,
  author={Cao, Keyan and Liu, Yefan and Meng, Gongjie and Sun, Qimeng},
  journal={IEEE Access}, 
  title={An Overview on Edge Computing Research}, 
  year={2020},
  volume={8},
  number={},
  pages={85714-85728},
  keywords={Cloud computing;Edge computing;Real-time systems;Internet of Things;Bandwidth;Security;Data privacy;Edge computing;cloud computing;Internet of Things},
  doi={10.1109/ACCESS.2020.2991734}}

@ARTICLE{edgeCompTerm,
  author={Satyanarayanan, Mahadev},
  journal={Computer}, 
  title={The Emergence of Edge Computing}, 
  year={2017},
  volume={50},
  number={1},
  pages={30-39},
  keywords={Investments;Edge computing;Cloud computing;Augmented reality;Internet of things;Computer vision;Content management;Data analysis;cloud;mobile;cloudlets;networking;edge computing;fog computing;augmented reality;AR;virtual reality;VR;Internet of Things;IoT;pervasive computing;computer vision;data analytics;content delivery networks;security;privacy;telecommunications;cognitive assistance;GigaSight;Outlook},
  doi={10.1109/MC.2017.9}}

@ARTICLE{edgeCompDis,
  author={Mao, Yuyi and You, Changsheng and Zhang, Jun and Huang, Kaibin and Letaief, Khaled B.},
  journal={IEEE Communications Surveys {\&} Tutorials}, 
  title={A Survey on Mobile Edge Computing: The Communication Perspective}, 
  year={2017},
  volume={19},
  number={4},
  pages={2322-2358},
  keywords={Cloud computing;Edge computing;5G mobile communication;Mobile computing;Wireless communication;Mobile edge computing;fog computing;mobile cloud computing;computation offloading;resource management;green computing},
  doi={10.1109/COMST.2017.2745201}}


@ARTICLE{iot,
  author={Al-Fuqaha, Ala and Guizani, Mohsen and Mohammadi, Mehdi and Aledhari, Mohammed and Ayyash, Moussa},
  journal={IEEE Communications Surveys {\&} Tutorials}, 
  title={Internet of Things: A Survey on Enabling Technologies, Protocols, and Applications}, 
  year={2015},
  volume={17},
  number={4},
  pages={2347-2376},
  keywords={Internet of things;Computer architecture;Radiofrequency identification;Intelligent sensors;Mobile communication;Internet of things;IoT;CoAP;MQTT;AMQP;XMPP;DDS;mDNS;IoT Gateway;Internet of Things (IoT);CoAP;MQTT;AMQP;XMPP;DDS;mDNS;IoT gateway},
  doi={10.1109/COMST.2015.2444095}}

@article{smartCity,
  title={Smart cities: Opportunities, challenges, and security threats},
  author={Khalifa, Ehab},
  journal={Journal of Strategic Innovation and Sustainability},
  volume={14},
  number={3},
  year={2019}
}

@misc{plat, author={Khalifa, Ehab}, url={https://www.hyundai.com/id/en/hyundai-story/articles/arti-plat-nomor-putih-dan-kapan-pemberlakuan-plat-putih-kendaraan-0000000187}, journal={HYUNDAI MOTORS}, year={2024}} 

@inproceedings{chameleon,
author = {Jiang, Junchen and Ananthanarayanan, Ganesh and Bodik, Peter and Sen, Siddhartha and Stoica, Ion},
title = {Chameleon: scalable adaptation of video analytics},
year = {2018},
isbn = {9781450355674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230543.3230574},
doi = {10.1145/3230543.3230574},
abstract = {Applying deep convolutional neural networks (NN) to video data at scale poses a substantial systems challenge, as improving inference accuracy often requires a prohibitive cost in computational resources. While it is promising to balance resource and accuracy by selecting a suitable NN configuration (e.g., the resolution and frame rate of the input video), one must also address the significant dynamics of the NN configuration's impact on video analytics accuracy. We present Chameleon, a controller that dynamically picks the best configurations for existing NN-based video analytics pipelines. The key challenge in Chameleon is that in theory, adapting configurations frequently can reduce resource consumption with little degradation in accuracy, but searching a large space of configurations periodically incurs an overwhelming resource overhead that negates the gains of adaptation. The insight behind Chameleon is that the underlying characteristics (e.g., the velocity and sizes of objects) that affect the best configuration have enough temporal and spatial correlation to allow the search cost to be amortized over time and across multiple video feeds. For example, using the video feeds of five traffic cameras, we demonstrate that compared to a baseline that picks a single optimal configuration offline, Chameleon can achieve 20-50\% higher accuracy with the same amount of resources, or achieve the same accuracy with only 30--50\% of the resources (a 2-3X speedup).},
booktitle = {Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication},
pages = {253–266},
numpages = {14},
keywords = {video analytics, object detection, deep neural networks},
location = {Budapest, Hungary},
series = {SIGCOMM '18}
}

@INPROCEEDINGS{jcab,
  author={Wang, Can and Zhang, Sheng and Chen, Yu and Qian, Zhuzhong and Wu, Jie and Xiao, Mingjun},
  booktitle={IEEE INFOCOM 2020 - IEEE Conference on Computer Communications}, 
  title={Joint Configuration Adaptation and Bandwidth Allocation for Edge-based Real-time Video Analytics}, 
  year={2020},
  volume={},
  number={},
  pages={257-266},
  abstract={Real-time analytics on video data demands intensive computation resources and high energy consumption. Traditional cloud-based video analytics relies on large centralized clusters to ingest video streams. With edge computing, we can offload compute-intensive analysis tasks to the nearby server, thus mitigating long latency incurred by data transmission via wide area networks. When offloading frames from the front-end device to the edge server, the application configuration (frame sampling rate and frame resolution) will impact several metrics, such as energy consumption, analytics accuracy and user-perceived latency. In this paper, we study the configuration adaption and bandwidth allocation for multiple video streams, which are connected to the same edge node sharing an upload link. We propose an efficient online algorithm, called JCAB, which jointly optimizes configuration adaption and bandwidth allocation to address a number of key challenges in edge-based video analytics systems, including edge capacity limitation, unknown network variation, intrusive dynamics of video contents. Our algorithm is developed based on Lyapunov optimization and Markov approximation, works online without requiring future information, and achieves a provable performance bound. Simulation results show that JCAB can effectively balance the analytics accuracy and energy consumption while keeping low system latency.},
  keywords={Streaming media;Bandwidth;Energy consumption;Servers;Channel allocation;Image resolution;Energy resolution},
  doi={10.1109/INFOCOM41043.2020.9155524},
  ISSN={2641-9874},
  month={July},}


@INPROCEEDINGS{automl,
  author={Galanopoulos, Apostolos and Ayala-Romero, Jose A. and Leith, Douglas J. and Iosifidis, George},
  booktitle={IEEE INFOCOM 2021 - IEEE Conference on Computer Communications}, 
  title={AutoML for Video Analytics with Edge Computing}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  keywords={Performance evaluation;Machine learning algorithms;Handheld computers;Visual analytics;Wireless networks;Roads;Conferences;Edge Computing;Online Learning;GP-UCB},
  doi={10.1109/INFOCOM42981.2021.9488704}}


@ARTICLE{killer,
  author={Ananthanarayanan, Ganesh and Bahl, Paramvir and Bodík, Peter and Chintalapudi, Krishna and Philipose, Matthai and Ravindranath, Lenin and Sinha, Sudipta},
  journal={Computer}, 
  title={Real-Time Video Analytics: The Killer App for Edge Computing}, 
  year={2017},
  volume={50},
  number={10},
  pages={58-67},
  keywords={Cameras;Streaming media;Cloud computing;Video analytics;Automobiles;Bandwidth;Surveillance;real-time video analytics;real time;video analytics;edge computing;latency;bandwidth;smart cameras;provisioning;cameras;camera networks;intelligent edge},
  doi={10.1109/MC.2017.3641638}}

@misc{middleware, url={https://www.ibm.com/topics/middleware}, journal={IBM}, year={2021}, month={Oct}, author={IBM}} 

@inproceedings{reducto,
author = {Li, Yuanqi and Padmanabhan, Arthi and Zhao, Pengzhan and Wang, Yufei and Xu, Guoqing Harry and Netravali, Ravi},
title = {Reducto: On-Camera Filtering for Resource-Efficient Real-Time Video Analytics},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405874},
doi = {10.1145/3387514.3405874},
abstract = {To cope with the high resource (network and compute) demands of real-time video analytics pipelines, recent systems have relied on frame filtering. However, filtering has typically been done with neural networks running on edge/backend servers that are expensive to operate. This paper investigates on-camera filtering, which moves filtering to the beginning of the pipeline. Unfortunately, we find that commodity cameras have limited compute resources that only permit filtering via frame differencing based on low-level video features. Used incorrectly, such techniques can lead to unacceptable drops in query accuracy. To overcome this, we built Reducto, a system that dynamically adapts filtering decisions according to the time-varying correlation between feature type, filtering threshold, query accuracy, and video content. Experiments with a variety of videos and queries show that Reducto achieves significant (51-97\% of frames) filtering benefits, while consistently meeting the desired accuracy.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {359–376},
numpages = {18},
keywords = {video analytics, object detection, deep neural networks},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{glimpse,
author = {Chen, Tiffany Yu-Han and Ravindranath, Lenin and Deng, Shuo and Bahl, Paramvir and Balakrishnan, Hari},
title = {Glimpse: Continuous, Real-Time Object Recognition on Mobile Devices},
year = {2015},
isbn = {9781450336314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2809695.2809711},
doi = {10.1145/2809695.2809711},
abstract = {Glimpse is a continuous, real-time object recognition system for camera-equipped mobile devices. Glimpse captures full-motion video, locates objects of interest, recognizes and labels them, and tracks them from frame to frame for the user. Because the algorithms for object recognition entail significant computation, Glimpse runs them on server machines. When the latency between the server and mobile device is higher than a frame-time, this approach lowers object recognition accuracy. To regain accuracy, Glimpse uses an active cache of video frames on the mobile device. A subset of the frames in the active cache are used to track objects on the mobile, using (stale) hints about objects that arrive from the server from time to time. To reduce network bandwidth usage, Glimpse computes trigger frames to send to the server for recognizing and labeling. Experiments with Android smartphones and Google Glass over Verizon, AT&T, and a campus Wi-Fi network show that with hardware face detection support (available on many mobile devices), Glimpse achieves precision between 96.4\% to 99.8\% for continuous face recognition, which improves over a scheme performing hardware face detection and server-side recognition without Glimpse's techniques by between 1.8-2.5\texttimes{}. The improvement in precision for face recognition without hardware detection is between 1.6-5.5\texttimes{}. For road sign recognition, which does not have a hardware detector, Glimpse achieves precision between 75\% and 80\%; without Glimpse, continuous detection is non-functional (0.2\%-1.9\% precision).},
booktitle = {Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems},
pages = {155–168},
numpages = {14},
keywords = {caching, cloud computing, google glass, mobile computing, wearable computing},
location = {Seoul, South Korea},
series = {SenSys '15}
}

@misc{grpc,
publisher = {grpc.io},
author = {{gRPC}},
title = {gRPC - A high performance, open source universal RPC framework},
year = {2024}}

@misc{protobuf,
publisher = {protobuf.dev},
author = {{Protocol Buffers Documentation}},
title = {protobuf - Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data},
year = {2024}}


@misc{python,
publisher = {Python.org},
author = {{Python}},
title = {What is Python?},
year = {2024}}

@misc{js,
publisher = {developer.mozilla.org},
author = {{MDN}},
title = {What is JavaScript? - Learn web development},
year = {2024}}

@misc{jsDisad,
publisher = {codingninjas.com},
author = {{codingninjas}},
title = {Advantages and Disadvantages of Node.js - Coding Ninjas},
year = {2024}}

% Online vs Offline Machine Learning – What’s the Difference?

@misc{offlineOnline,
publisher = {qwak.com},
author = {{qwak}},
title = {Online vs Offline Machine Learning - What's the Difference?},
year = {2024}}


@misc{online,
publisher = {baeldung.com},
author = {{Budu, Emmanuella}},
title = {Online Learning vs. Offline Learning},
year = {2024}}